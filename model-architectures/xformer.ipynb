{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25793a95-4c82-4cd3-b13e-f73c120ec6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xformers.factory.model_factory import xFormer, xFormerConfig\n",
    "import torch\n",
    "\n",
    "EMB = 384\n",
    "SEQ = 1024\n",
    "BATCH = 16\n",
    "VOCAB = 64\n",
    "\n",
    "my_config = [\n",
    "    # A list of the encoder or decoder blocks which constitute the Transformer.\n",
    "    # Note that a sequence of different encoder blocks can be used, same for decoders\n",
    "    {\n",
    "        \"reversible\": False,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"encoder\",\n",
    "        \"num_layers\": 3,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        # \"position_encoding_config\": {\n",
    "        #     \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "        #     \"seq_len\": 1024,\n",
    "        #     \"vocab_size\": VOCAB,\n",
    "        # },\n",
    "        \"multi_head_config\": {\n",
    "            \"num_heads\": 4,\n",
    "            \"residual_dropout\": 0.1,\n",
    "            \"attention\": {\n",
    "                \"name\": \"scaled_dot_product\",  # whatever attention mechanism\n",
    "                \"dropout\": 0.1,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": SEQ,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"FusedMLP\",\n",
    "            \"dropout\": 0.1,\n",
    "            \"activation\": \"gelu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# This part of xFormers is entirely type checked and needs a config object,\n",
    "# could be changed in the future\n",
    "config = xFormerConfig(my_config)\n",
    "model = xFormer.from_config(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f00dda-892c-45fe-b5eb-653ca0e6583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Test out with dummy inputs\n",
    "# x = (torch.rand((BATCH, SEQ)) * VOCAB).abs().to(torch.int)\n",
    "\n",
    "x = torch.rand((BATCH, SEQ, EMB))\n",
    "y = model(src=x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c519b085-28e7-4f67-a22a-30ebfc979b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoders.0.wrap_att.layer.norm.weight\n",
      "encoders.0.wrap_att.layer.norm.bias\n",
      "encoders.0.wrap_att.layer.sublayer.in_proj_container.q_proj.weight\n",
      "encoders.0.wrap_att.layer.sublayer.in_proj_container.q_proj.bias\n",
      "encoders.0.wrap_att.layer.sublayer.in_proj_container.k_proj.weight\n",
      "encoders.0.wrap_att.layer.sublayer.in_proj_container.k_proj.bias\n",
      "encoders.0.wrap_att.layer.sublayer.in_proj_container.v_proj.weight\n",
      "encoders.0.wrap_att.layer.sublayer.in_proj_container.v_proj.bias\n",
      "encoders.0.wrap_att.layer.sublayer.proj.weight\n",
      "encoders.0.wrap_att.layer.sublayer.proj.bias\n",
      "encoders.0.wrap_ff.layer.norm.weight\n",
      "encoders.0.wrap_ff.layer.norm.bias\n",
      "encoders.0.wrap_ff.layer.sublayer.mlp.0.weight\n",
      "encoders.0.wrap_ff.layer.sublayer.mlp.2.weight\n",
      "encoders.1.wrap_att.layer.norm.weight\n",
      "encoders.1.wrap_att.layer.norm.bias\n",
      "encoders.1.wrap_att.layer.sublayer.in_proj_container.q_proj.weight\n",
      "encoders.1.wrap_att.layer.sublayer.in_proj_container.q_proj.bias\n",
      "encoders.1.wrap_att.layer.sublayer.in_proj_container.k_proj.weight\n",
      "encoders.1.wrap_att.layer.sublayer.in_proj_container.k_proj.bias\n",
      "encoders.1.wrap_att.layer.sublayer.in_proj_container.v_proj.weight\n",
      "encoders.1.wrap_att.layer.sublayer.in_proj_container.v_proj.bias\n",
      "encoders.1.wrap_att.layer.sublayer.proj.weight\n",
      "encoders.1.wrap_att.layer.sublayer.proj.bias\n",
      "encoders.1.wrap_ff.layer.norm.weight\n",
      "encoders.1.wrap_ff.layer.norm.bias\n",
      "encoders.1.wrap_ff.layer.sublayer.mlp.0.weight\n",
      "encoders.1.wrap_ff.layer.sublayer.mlp.2.weight\n",
      "encoders.2.wrap_att.layer.norm.weight\n",
      "encoders.2.wrap_att.layer.norm.bias\n",
      "encoders.2.wrap_att.layer.sublayer.in_proj_container.q_proj.weight\n",
      "encoders.2.wrap_att.layer.sublayer.in_proj_container.q_proj.bias\n",
      "encoders.2.wrap_att.layer.sublayer.in_proj_container.k_proj.weight\n",
      "encoders.2.wrap_att.layer.sublayer.in_proj_container.k_proj.bias\n",
      "encoders.2.wrap_att.layer.sublayer.in_proj_container.v_proj.weight\n",
      "encoders.2.wrap_att.layer.sublayer.in_proj_container.v_proj.bias\n",
      "encoders.2.wrap_att.layer.sublayer.proj.weight\n",
      "encoders.2.wrap_att.layer.sublayer.proj.bias\n",
      "encoders.2.wrap_ff.layer.norm.weight\n",
      "encoders.2.wrap_ff.layer.norm.bias\n",
      "encoders.2.wrap_ff.layer.sublayer.mlp.0.weight\n",
      "encoders.2.wrap_ff.layer.sublayer.mlp.2.weight\n"
     ]
    }
   ],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semafor_nlg",
   "language": "python",
   "name": "semafor_nlg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
